{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c0eb07",
   "metadata": {},
   "source": [
    "# Final: Classification with YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3316b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21d5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "random_seed = 42\n",
    "name = f\"seed{random_seed}\"\n",
    "custom_epochs = 40\n",
    "\n",
    "# source folder and classes\n",
    "source_dir = Path(\"/home/shared-data/corrosion_images\")\n",
    "classes = [\"corrosion\", \"no_corrosion\"]\n",
    "\n",
    "# new folder structure to be created\n",
    "output_dir = Path(\"/home/liva/classification_dataset\")\n",
    "train_dir = output_dir / \"train\"\n",
    "val_dir = output_dir / \"val\"\n",
    "test_dir = output_dir / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2bd09",
   "metadata": {},
   "source": [
    "## 1. Create Train/Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1de0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 2799 images\n",
      "Val set: 599 images\n",
      "Test set: 601 images\n"
     ]
    }
   ],
   "source": [
    "# create directories\n",
    "for split in [train_dir, val_dir, test_dir]:\n",
    "    for cls in classes:\n",
    "        (split / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# split and copy images\n",
    "for cls in classes:\n",
    "    cls_path = source_dir / cls\n",
    "    images = (\n",
    "        list(cls_path.glob(\"*.jpg\")) +\n",
    "        list(cls_path.glob(\"*.png\")) +\n",
    "        list(cls_path.glob(\"*.jpeg\"))\n",
    "    )\n",
    "\n",
    "    # 70% train, 30% temp\n",
    "    train_imgs, temp_imgs = train_test_split(\n",
    "        images,\n",
    "        test_size=0.3,\n",
    "        random_state=random_seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # split temp into val and test (15% each)\n",
    "    val_imgs, test_imgs = train_test_split(\n",
    "        temp_imgs,\n",
    "        test_size=0.5,\n",
    "        random_state=random_seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # copy train images\n",
    "    for img in train_imgs:\n",
    "        shutil.copy2(img, train_dir / cls / img.name)\n",
    "\n",
    "    # copy validation images\n",
    "    for img in val_imgs:\n",
    "        shutil.copy2(img, val_dir / cls / img.name)\n",
    "\n",
    "    # copy test images\n",
    "    for img in test_imgs:\n",
    "        shutil.copy2(img, test_dir / cls / img.name)\n",
    "\n",
    "# counts\n",
    "print(f\"Train set: {len(list(train_dir.rglob('*.*')))} images\")\n",
    "print(f\"Val set: {len(list(val_dir.rglob('*.*')))} images\")\n",
    "print(f\"Test set: {len(list(test_dir.rglob('*.*')))} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60ed7e",
   "metadata": {},
   "source": [
    "## Create Funciton to Save Training and Validation Loss Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd3a57",
   "metadata": {},
   "source": [
    "I will now create a function, that will plot our training and validation loss when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a15ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_loss(csv_path, title=None, save_path=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if title is None:\n",
    "        title = \"Training vs Validation Loss\"\n",
    "\n",
    "    # create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.plot(df['epoch'], df['train/loss'], label='Training Loss')\n",
    "    ax.plot(df['epoch'], df['val/loss'], label='Validation Loss')\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    # save plot\n",
    "    if save_path:\n",
    "        if not save_path.lower().endswith(\".png\"):\n",
    "            save_path += \".png\"\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f65055",
   "metadata": {},
   "source": [
    "## Baseline Highway data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee61e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.58%\n",
      "Confusion Matrix:\n",
      " [[153 149]\n",
      " [136 163]]\n"
     ]
    }
   ],
   "source": [
    "# classes\n",
    "classes = [\"no_corrosion\", \"corrosion\"]\n",
    "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# initialize lists\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for cls in classes:\n",
    "    cls_folder = Path(test_dir) / cls\n",
    "    for img_path in cls_folder.glob(\"*\"):\n",
    "        if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            images.append(str(img_path))\n",
    "            labels.append(class_to_idx[cls])\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# generating random predictions\n",
    "np.random.seed(42)\n",
    "random_preds = np.random.randint(0, len(classes), size=len(images))\n",
    "\n",
    "# calculating accuracy and creating confusion matrix\n",
    "accuracy = accuracy_score(labels, random_preds)\n",
    "cm = confusion_matrix(labels, random_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c40b99",
   "metadata": {},
   "source": [
    "Baseline Accuracy is 52.58%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3f5aa",
   "metadata": {},
   "source": [
    "## Training Final YOLOv8s Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87e34c",
   "metadata": {},
   "source": [
    "For that, I need new epochs, as I will train with 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481febb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K      29/40      2.34G    0.02661         46        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 2.3it/s 19.3s0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 20.3it/s 0.2s.1s\n",
      "                   all      0.983          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      30/40      2.34G    0.01652         64        256: 73% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ 32/44 10.4it/s 13.9s<1.2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K      30/40      2.34G     0.0173         46        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 2.2it/s 20.1s0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 20.2it/s 0.2s.1s\n",
      "                   all      0.987          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      31/40      2.34G    0.01229         64        256: 52% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 23/44 10.7it/s 10.6s<2.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid SOS parameters for sequential JPEG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K      31/40      2.34G    0.01212         46        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 2.1it/s 21.1s0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 20.0it/s 0.3s.1s\n",
      "                   all      0.985          1\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 21, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "31 epochs completed in 0.176 hours.\n",
      "Optimizer stripped from /home/liva/runs/classify/0000seed42_40epochs_yoloS_final/weights/last.pt, 10.3MB\n",
      "Optimizer stripped from /home/liva/runs/classify/0000seed42_40epochs_yoloS_final/weights/best.pt, 10.3MB\n",
      "\n",
      "Validating /home/liva/runs/classify/0000seed42_40epochs_yoloS_final/weights/best.pt...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.10.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14916MiB)\n",
      "YOLOv8s-cls summary (fused): 30 layers, 5,077,762 parameters, 0 gradients, 12.4 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liva/classification_dataset/train... found 2799 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/liva/classification_dataset/val... found 599 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /home/liva/classification_dataset/test... found 601 images in 2 classes ‚úÖ \n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 17.5it/s 0.3s.1s\n",
      "                   all       0.99          1\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/liva/runs/classify/0000seed42_40epochs_yoloS_final\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid SOS parameters for sequential JPEG\n"
     ]
    }
   ],
   "source": [
    "model_small_final = YOLO(\"yolov8s-cls.pt\") # small\n",
    "results_small = model_small_final.train(\n",
    "    data=output_dir,\n",
    "    epochs=custom_epochs,\n",
    "    imgsz=256,\n",
    "    batch=64,\n",
    "    device='0',\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    dropout=0.2,\n",
    "    patience=10,\n",
    "    name=f\"0000{name}_{custom_epochs}epochs_yoloS_final\",\n",
    "    seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cea18f",
   "metadata": {},
   "source": [
    "The model stopped earlier (epoch 31) due to patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b47eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to: output/yolo_finalmodel.png\n"
     ]
    }
   ],
   "source": [
    "plot_train_val_loss(\"runs/classify/0000seed42_40epochs_yoloS_final/results.csv\", title=\"Training vs Validation Loss with YOLOv8\", save_path=\"output/yolo_finalmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b86e95",
   "metadata": {},
   "source": [
    "Now, we want to look at the accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"/home/liva/runs/classify/0000seed42_40epochs_yoloS_final/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7603471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.217 üöÄ Python-3.10.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14916MiB)\n",
      "YOLOv8s-cls summary (fused): 30 layers, 5,077,762 parameters, 0 gradients, 12.4 GFLOPs\n",
      "WARNING ‚ö†Ô∏è Dataset 'split=train' not found at /home/liva/classification_dataset/test/train\n",
      "Found 601 images in subdirectories. Attempting to split...\n",
      "Splitting /home/liva/classification_dataset/test (2 classes, 601 images) into 80% train, 20% val...\n",
      "Split complete in /home/liva/classification_dataset/test_split ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liva/classification_dataset/test_split/train... found 601 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/liva/classification_dataset/test_split/val... found 502 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3390.8¬±1149.5 MB/s, size: 711.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liva/classification_dataset/test_split/val... 502 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 502/502 2.1Kit/s 0.2s0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/liva/classification_dataset/test_split/val.cache\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 6.6it/s 4.9s<0.0s\n",
      "                   all      0.982          1\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/liva/runs/classify/val18\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7f060ceae5f0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.9910358488559723\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.9820716977119446, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9910358488559723}\n",
       "save_dir: PosixPath('/home/liva/runs/classify/val18')\n",
       "speed: {'preprocess': 0.12125013269988663, 'inference': 1.0163670042121553, 'loss': 0.0010101770262319254, 'postprocess': 0.001265982767025313}\n",
       "task: 'classify'\n",
       "top1: 0.9820716977119446\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val(data=test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510f63b",
   "metadata": {},
   "source": [
    "The accuracy of this model is 0.9835. (might be slightly different in above output (top1))\n",
    "Comparing that to the Baseline Accuracy of 52.58%, we have a much better classification model.\n",
    "\n",
    "But this data is still only the highway training data. Our use-case is corrosion damages in ports and waterways, therefore I will test how well this training set is holding up against a test set of use-case data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f944a",
   "metadata": {},
   "source": [
    "## Applying our model on Use-Case Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.217 üöÄ Python-3.10.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14916MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8s-cls summary (fused): 30 layers, 5,077,762 parameters, 0 gradients, 12.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4371.1¬±1274.6 MB/s, size: 6029.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liva/use_case_imgs... 73 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 73/73 1.6Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/liva/use_case_imgs.cache\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 1.2it/s 4.3s0.2ss\n",
      "                   all      0.795          1\n",
      "Speed: 1.3ms preprocess, 3.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/liva/runs/classify/val20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"/home/liva/runs/classify/0000seed42_40epochs_yoloS_final/weights/best.pt\")\n",
    "# evaluate on the use case images\n",
    "metrics = model.val(data=\"/home/liva/use_case_usecase.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041357b",
   "metadata": {},
   "source": [
    "The use-case accuracy is 0.795."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d6e7b",
   "metadata": {},
   "source": [
    "## Baseline Harbor Use-Case Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f899c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.27%\n",
      "Confusion Matrix:\n",
      " [[11  4]\n",
      " [25 33]]\n"
     ]
    }
   ],
   "source": [
    "use_case_dir = \"/home/liva/use_case_imgs\"\n",
    "\n",
    "# initialize lists\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for cls in classes:\n",
    "    cls_folder = Path(use_case_dir) / cls\n",
    "    for img_path in cls_folder.glob(\"*\"):\n",
    "        if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            images.append(str(img_path))\n",
    "            labels.append(class_to_idx[cls])\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# generating random predictions\n",
    "np.random.seed(42)\n",
    "random_preds = np.random.randint(0, len(classes), size=len(images))\n",
    "\n",
    "# calculating accuracy and creating confusion matrix\n",
    "accuracy = accuracy_score(labels, random_preds)\n",
    "cm = confusion_matrix(labels, random_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c130fc",
   "metadata": {},
   "source": [
    "Comparing the 79.5% accuracy of the model to the baseline results of 60.27% shows that the model classifies significantly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b89a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
