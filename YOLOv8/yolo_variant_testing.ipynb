{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c0eb07",
   "metadata": {},
   "source": [
    "# Classification with YOLOv8\n",
    "#### Variant Testing: Choosing whether to use YOLOV8N, YOLOV8S or YOLOV8M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3316b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "random_seed = 42\n",
    "custom_epochs = 20\n",
    "\n",
    "# source folder and classes\n",
    "source_dir = Path(\"/home/shared-data/corrosion_images\")\n",
    "classes = [\"corrosion\", \"no_corrosion\"]\n",
    "\n",
    "# new folder structure to be created\n",
    "output_dir = Path(\"/home/liva/dataset_variants\")\n",
    "train_dir = output_dir / \"train\"\n",
    "val_dir = output_dir / \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2bd09",
   "metadata": {},
   "source": [
    "## 1. Create Train/Val/Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the directories\n",
    "for split in [train_dir, val_dir]:\n",
    "    for cls in classes:\n",
    "        (split / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# split and copy images for each class\n",
    "for cls in classes:\n",
    "    cls_path = source_dir / cls\n",
    "    images = list(cls_path.glob(\"*.jpg\")) + list(cls_path.glob(\"*.png\")) + list(cls_path.glob(\"*.jpeg\"))\n",
    "    \n",
    "    # 80/20 for training and validation\n",
    "    train_imgs, val_imgs = train_test_split(\n",
    "        images, \n",
    "        test_size=0.2, \n",
    "        random_state=random_seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # copying to training\n",
    "    for img in train_imgs:\n",
    "        shutil.copy2(img, train_dir / cls / img.name)\n",
    "    \n",
    "    # copying to validation\n",
    "    for img in val_imgs:\n",
    "        shutil.copy2(img, val_dir / cls / img.name)\n",
    "\n",
    "print(f\"Train set: {len(list(train_dir.rglob('*.*')))} images\")\n",
    "print(f\"Val set: {len(list(val_dir.rglob('*.*')))} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03392c0",
   "metadata": {},
   "source": [
    "Yolov8 has multiple models. They range from nano, to small, to medium and to others (not experimented with in this project).\n",
    "I will check out these three and evaluate their results in the project resport.\n",
    "\n",
    "Below: set random seed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "name = f\"seed{random_seed}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ddd2e",
   "metadata": {},
   "source": [
    "## Nano YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 999\n",
    "model_nano = YOLO(\"yolov8n-cls.pt\")\n",
    "results = model_nano.train(\n",
    "    data=output_dir,\n",
    "    epochs=custom_epochs,\n",
    "    imgsz=256,\n",
    "    batch=64,\n",
    "    device='0',\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    dropout=0.2,\n",
    "    name=f\"{name}_{custom_epochs}epochs_yoloN\",\n",
    "    seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a9c7faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.217 üöÄ Python-3.10.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14916MiB)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,437,442 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liva/dataset/train... found 3199 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/liva/dataset/val... found 800 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2997.7¬±1343.5 MB/s, size: 1310.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liva/dataset/val... 800 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 666.4Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/corrosion/6074246.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/no_corrosion/16137439.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/no_corrosion/6067519.jpg: corrupt JPEG restored and saved\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50/50 6.5it/s 7.7s0.1s\n",
      "                   all      0.988          1\n",
      "Speed: 0.1ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/liva/runs/classify/val47\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get metrics\n",
    "metrics_nano = model_nano.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab18c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of YOLOv8n on Seed 42: 0.987500011920929\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of YOLOv8N\", metrics_nano.top1)\n",
    "# plot_train_val_loss(\"runs/classify/seed42_20epochs_yoloN/results.csv\", title=\"Training vs Validation Loss, YOLOv8n\", save_path=\"output/seed42_20epochs_yoloN_AAA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448825e",
   "metadata": {},
   "source": [
    "## Small YOLOv8S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72665e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = YOLO(\"yolov8s-cls.pt\") # small\n",
    "results1 = model_small.train(\n",
    "    data=output_dir,\n",
    "    epochs=custom_epochs,\n",
    "    imgsz=256,\n",
    "    batch=64,\n",
    "    device='0',\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    dropout=0.2,\n",
    "    name=f\"{name}_{custom_epochs}epochs_yoloS\",\n",
    "    seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c20671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.217 üöÄ Python-3.10.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14916MiB)\n",
      "YOLOv8s-cls summary (fused): 30 layers, 5,077,762 parameters, 0 gradients, 12.4 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liva/dataset/train... found 3199 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/liva/dataset/val... found 800 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4462.7¬±1409.5 MB/s, size: 1310.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liva/dataset/val... 800 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 966.4Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/corrosion/6074246.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/no_corrosion/16137439.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/no_corrosion/6067519.jpg: corrupt JPEG restored and saved\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50/50 6.3it/s 7.9s<0.0s\n",
      "                   all      0.988          1\n",
      "Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/liva/runs/classify/val48\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get metrics\n",
    "metrics_small = model_small.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ca56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of YOLOv8s on Seed 42: 0.987500011920929\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of YOLOv8S:\", metrics_small.top1)\n",
    "# plot_train_val_loss(\"runs/classify/seed42_20epochs_yoloS/results.csv\", title=\"Training vs Validation Loss, YOLOv8s\", save_path=\"output/seed42_20epochs_yoloS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57e70caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_train_val_loss(\"runs/classify/seed42_20epochs_yoloS/results.csv\", title=\"Training vs Validation Loss, YOLOv8s\", save_path=\"output/seed42_20epochs_yoloS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06470907",
   "metadata": {},
   "source": [
    "## Medium YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1013eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_medium = YOLO(\"yolov8m-cls.pt\") # medium\n",
    "results2 = model_medium.train(\n",
    "    data=output_dir,\n",
    "    epochs=custom_epochs,\n",
    "    imgsz=256,\n",
    "    batch=64,\n",
    "    device='0',\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    dropout=0.2,\n",
    "    name=f\"{name}_{custom_epochs}epochs_yoloM\",\n",
    "    seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bbcffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.217 üöÄ Python-3.10.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14916MiB)\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,765,218 parameters, 0 gradients, 41.6 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liva/dataset/train... found 3199 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/liva/dataset/val... found 800 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4700.6¬±1268.1 MB/s, size: 1310.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liva/dataset/val... 800 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 926.2Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/corrosion/6074246.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/no_corrosion/16137439.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/liva/dataset/val/no_corrosion/6067519.jpg: corrupt JPEG restored and saved\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50/50 6.1it/s 8.2s<0.1s\n",
      "                   all       0.99          1\n",
      "Speed: 0.1ms preprocess, 1.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/liva/runs/classify/val49\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get metrics\n",
    "metrics_medium = model_medium.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fccb0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of YOLOv8m on Seed 42: 0.9900000095367432\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of YOLOv8M:\", metrics_medium.top1)\n",
    "# plot_train_val_loss(\"runs/classify/seed42_20epochs_yoloM/results.csv\", title=\"Training vs Validation Loss, YOLOv8m\", save_path=\"output/seed42_20epochs_yoloM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319a996",
   "metadata": {},
   "source": [
    "The best result is with the model YOLOv8S; small. Further explanations are found in the project report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
